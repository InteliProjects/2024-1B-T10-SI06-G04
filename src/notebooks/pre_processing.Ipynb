{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ambiente de Configuração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir variáveis de Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "isAtGoogleColab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%pip install scikit-learn\n",
    "%pip install keras\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install google-colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import spacy.cli\n",
    "import re\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando dados do spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "spacy.cli.download('en_core_web_lg')\n",
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Dados de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if isAtGoogleColab:\n",
    "    df = pd.read_csv('/content/classification-labeled.csv', encoding='latin1', delimiter=';')\n",
    "else:\n",
    "    df = pd.read_csv('../data/classification-labeled.csv', encoding='latin1', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Pré Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo links dos comentários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `extract_links(comment)` serve para extrair URLs de um texto fornecido e remover esses links do texto original, devolvendo uma tupla com a lista de links encontrados e o texto já limpo. Seguindo esse processo, a função `extract_links_df(df_recived)` aplica a `extract_links` em cada entrada da coluna 'comment' de um DataFrame, armazenando os resultados em novas colunas para os links extraídos e os comentários limpos. Por fim, o DataFrame é reorganizado para incluir apenas as colunas 'id', 'comment', 'links' e 'sentiment', retornando assim a versão modificada do DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(comment):\n",
    "    \"\"\"\n",
    "    Extract links from a phrase\n",
    "    Args:\n",
    "      String: a string\n",
    "\n",
    "    Returns:\n",
    "      A string with the phrase\n",
    "      A string with the links \n",
    "    \"\"\"\n",
    "    url_regex = r'https?://\\S+'\n",
    "    links = re.findall(url_regex, comment)\n",
    "    comment = re.sub(url_regex, '', comment)\n",
    "    return links, comment.strip()\n",
    "\n",
    "def extract_links_df(df_recived):\n",
    "    \"\"\"\n",
    "        Extract links from the df['comment']\n",
    "        Args:\n",
    "        Dataframe with a string column named 'comment'\n",
    "\n",
    "        Returns:\n",
    "        The modified dataframe\n",
    "    \"\"\"\n",
    "    df_recived['links'], df_recived['comment'] = zip(*df_recived['comment'].apply(extract_links))\n",
    "    df_recived = df_recived[['id','comment', 'links', 'sentiment']]\n",
    "    return df_recived\n",
    "\n",
    "#Test it\n",
    "df = extract_links_df(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converter as letras para minúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `to_lower(text)` é usada para transformar todas as letras maiúsculas de uma string fornecida em minúsculas, retornando a string modificada. Em complemento, a função `to_lower_df(df_lower)` aplica a `to_lower` na coluna 'comment' de um DataFrame, transformando todas as letras maiúsculas dessa coluna em minúsculas. Essa função retorna o DataFrame modificado com as alterações aplicadas na coluna especificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "    \"\"\"\n",
    "        Transform uppercase letters from a string into lowercase\n",
    "        Args:\n",
    "        A string\n",
    "\n",
    "        Returns:\n",
    "        A string\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def to_lower_df(df_lower):\n",
    "    \"\"\"\n",
    "        Transform uppercase letters from a dataframe into lowercase\n",
    "        Args:\n",
    "        Dataframe with colunm 'comment'\n",
    "\n",
    "        Returns:\n",
    "        The modified dataframe\n",
    "    \"\"\"\n",
    "    df_lower['comment'] = df_lower['comment'].apply(to_lower)\n",
    "    return df_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `tokenize_and_pre_processing(text)` é projetada para tokenizar e remover as palavras de parada de uma string fornecida, utilizando o modelo de linguagem do spaCy. Ela retorna uma lista de lemas filtrados que são entidades nomeadas ou pertencem às classes gramaticais substantivo, verbo, adjetivo ou advérbio. A função complementar `tokenize_and_pre_processing_df(df_tok)` aplica essa funcionalidade de processamento de texto na coluna 'comment' de um DataFrame, armazenando os tokens processados na coluna 'pos_tokens'. Essa função devolve o DataFrame atualizado com as modificações realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pre_processing(text):\n",
    "    \"\"\"\n",
    "        Tokenize, lemmatize and remove the stop words of a string\n",
    "        Args:\n",
    "        A string\n",
    "\n",
    "        Returns:\n",
    "        The modified string\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if (token.ent_type_ != '') or (token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV'])]\n",
    "    return lemmas\n",
    "\n",
    "def tokenize_and_pre_processing_df(df_tok):\n",
    "    \"\"\"\n",
    "        Tokenize, lemmatize and remove the stop words of a dataframe\n",
    "        Args:\n",
    "        Dataframe with a column named 'pos_tokens', which contains strings\n",
    "\n",
    "        Returns:\n",
    "        The modified dataframe\n",
    "    \"\"\"\n",
    "    df_tok['pos_tokens'] = df_tok['comment'].apply(tokenize_and_pre_processing)\n",
    "    return df_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remover Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `filter_tokens(tokens)` é destinada a manter apenas substantivos, verbos, adjetivos e advérbios de uma lista de tokens do spaCy, filtrando com base no tipo de entidade e classe gramatical. Ela retorna uma lista com os tokens filtrados que atendem esses critérios. Já a função `filter_tokens_df(df_filter)` aplica esse filtro na coluna 'pos_tokens' de um DataFrame, que contém listas de tokens do spaCy. A função retorna o DataFrame com a coluna 'pos_tokens' atualizada, contendo apenas os tokens filtrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    \"\"\"\n",
    "        Mantain only nouns, verbs, adjectives and adverbs from a spacy tokens list\n",
    "        Args:\n",
    "        Spacy tokens list\n",
    "\n",
    "        Returns:\n",
    "        A filtered list with the remaining tokens\n",
    "    \"\"\"\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if (token.ent_type_ != '') or (token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def filter_tokens_df(df_filter):\n",
    "    \"\"\"\n",
    "        Mantain only nouns, verbs, adjectives and adverbs from a dataframe\n",
    "        Args:\n",
    "        Dataframe with a column named 'pos_tokens', which contains a list of spacy tokens\n",
    "\n",
    "        Returns:\n",
    "        The modified dataframe\n",
    "    \"\"\"\n",
    "    df_filter['pos_tokens'] = df_filter['pos_tokens'].apply(filter_tokens)\n",
    "    return df_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `lemmatize(tokens)` extrai o lema de cada token em uma lista de tokens do spaCy, retornando uma lista de lemas. Complementando essa função, `lemmatize_df(df_lemmatize)` aplica a lematização na coluna 'pos_tokens' de um DataFrame, que contém listas de tokens do spaCy. Essa função atualiza o DataFrame substituindo os tokens originais pelos seus respectivos lemas na coluna 'pos_tokens' e retorna o DataFrame modificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    \"\"\"\n",
    "        Retain only the lemma from the tokens\n",
    "        Args:\n",
    "        Spacy tokens list\n",
    "\n",
    "        Returns:\n",
    "        A filtered list with the remaining tokens\n",
    "    \"\"\"\n",
    "    lemmas = [token.lemma_ for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "def lemmatize_df(df_lemmatize):\n",
    "    \"\"\"\n",
    "        Lemmatize the dataframe's column named 'pos_tokens'\n",
    "        Args:\n",
    "        Dataframe with a column named 'pos_tokens', which contains a list of spacy tokens\n",
    "\n",
    "        Returns:\n",
    "        The modified dataframe\n",
    "    \"\"\"\n",
    "    df_lemmatize['pos_tokens'] = df_lemmatize['pos_tokens'].apply(lemmatize)\n",
    "    return df_lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string `csv_data` define um CSV em formato texto com colunas separadas por ponto e vírgula, contendo os campos 'id', 'comment' e 'sentiment'. A função `pd.read_csv`, utilizando `StringIO`, é empregada para transformar essa string em um DataFrame, tratando a string como se fosse um arquivo CSV. Este método permite a carga direta dos dados em formato tabular dentro do DataFrame `df_teste`, organizando-os de acordo com as colunas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV to test\n",
    "csv_data = \"\"\"id;comment;sentiment\n",
    "1;That, my friend, is why The Mighty Swift Radio Cars of Stalybridge retain my costume.;0\n",
    "2;via The Guardian  Guardian front page, Monday 11 July 2022 - The #Uber files: Leak reveals secret lobbying operation to conquer the world  https://t.co/hjsUSc6AVZ;-1\n",
    "\"\"\"\n",
    "\n",
    "# Loading the test csv\n",
    "df_teste = pd.read_csv(StringIO(csv_data), sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_csv():\n",
    "    \"\"\"\n",
    "        Tests the loading of a CSV file into a pandas DataFrame.\n",
    "        This function attempts to load a CSV file named 'data.csv' from the current directory.\n",
    "        It asserts that the loaded DataFrame should not be empty.\n",
    "        \n",
    "        Args:\n",
    "        None: The function does not require external arguments.\n",
    "\n",
    "        Returns:\n",
    "        None: The function does not return a value but uses assertions and exception handling to ensure the correctness of the CSV loading.\n",
    "\n",
    "        Raises:\n",
    "            Asserts an error if the DataFrame is empty, which indicates the CSV file might be empty or missing.\n",
    "            Prints an error message if the CSV file could not be loaded or parsed, handling exceptions gracefully.\n",
    "\n",
    "        The function tries to load 'data.csv' into DataFrame 'df'. If the DataFrame 'df' is empty after loading,\n",
    "        an AssertionError is raised with the message \"DataFrame is empty\".\n",
    "        If there is an exception during loading, such as a FileNotFoundError or a parsing error, the exception is caught,\n",
    "        and an error message is printed, indicating the nature of the failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('data.csv')\n",
    "        assert not df.empty, \"DataFrame is empty\" \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load or parse CSV: {e}\") \n",
    "\n",
    "test_load_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_simple_plot():\n",
    "    \"\"\"\n",
    "        Tests the functionality of matplotlib to plot a simple graph.\n",
    "        This function plots y = x^2 using lists of x values and their squared y values.\n",
    "        It sets the title, labels for the X and Y axes, and ensures the plot environment is correctly set up.\n",
    "        The plot is not displayed but is closed immediately to check for exceptions during the plotting process.\n",
    "        \n",
    "        Args:\n",
    "        None: The function does not require external arguments.\n",
    "\n",
    "        Returns:\n",
    "        None: The function does not return a value but checks the plotting process for errors.\n",
    "\n",
    "        Raises:\n",
    "            Prints a success message if the plotting is successful.\n",
    "            Catches and prints an exception if the plotting fails, which helps identify issues in the plotting setup.\n",
    "\n",
    "        The function initializes lists 'x' with values [1, 2, 3, 4, 5] and 'y' with the squares of these values.\n",
    "        It then uses matplotlib to plot these values, setting the plot title and axis labels.\n",
    "        After setting up the plot, it is immediately closed using 'plt.close()', which is a crucial step in testing environments\n",
    "        to ensure that resources are properly released and that the plot setup itself does not cause errors.\n",
    "        If the plotting process completes without exceptions, a success message is printed. If an exception occurs,\n",
    "        it is caught and an appropriate error message is printed, indicating the failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = [1, 2, 3, 4, 5]\n",
    "        y = [xi**2 for xi in x]\n",
    "        plt.plot(x, y)\n",
    "        plt.title(\"Test Plot\")\n",
    "        plt.xlabel(\"X axis\")\n",
    "        plt.ylabel(\"Y axis\")\n",
    "        plt.close() \n",
    "        print(\"Plotting test passed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to plot: {e}\")\n",
    "\n",
    "test_simple_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def test_tokenization():\n",
    "    \"\"\"\n",
    "        Tests the tokenization functionality of the NLTK library.\n",
    "        This function first ensures the necessary 'punkt' tokenizer model is downloaded if not already present,\n",
    "        then tokenizes a predefined sentence, and finally asserts the number of tokens expected.\n",
    "        \n",
    "        Args:\n",
    "        None: The function does not require external arguments.\n",
    "\n",
    "        Returns:\n",
    "        None: The function does not return a value but checks for correct tokenization.\n",
    "\n",
    "        Raises:\n",
    "            Prints a success message if the test passes, indicating correct tokenization.\n",
    "            Catches and prints an exception if the test fails, providing details on what might have gone wrong.\n",
    "\n",
    "        The function initializes by downloading the 'punkt' model required for tokenization if it's not already downloaded.\n",
    "        It then defines a test sentence and uses the 'word_tokenize' function to tokenize this sentence.\n",
    "        The function asserts that the number of tokens should be exactly 7 based on the structure of 'test_sentence'.\n",
    "        If the assertion passes, it prints a success message. If there's an exception, such as an error in downloading\n",
    "        the model or incorrect tokenization, the exception is caught and an appropriate error message is printed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True) \n",
    "        test_sentence = \"Hello, this is a test sentence.\"  \n",
    "        tokens = word_tokenize(test_sentence)  \n",
    "        assert len(tokens) == 7, f\"Expected 7 tokens, got {len(tokens)}\"  \n",
    "        print(\"Tokenization test passed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization test failed: {e}\") \n",
    "\n",
    "test_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extract_links():\n",
    "    \"\"\"\n",
    "        Tests the extract_links_df function to ensure that it correctly removes links from a string.\n",
    "        Args:\n",
    "        None: The test does not require external arguments as the input data is defined within the function.\n",
    "\n",
    "        Returns:\n",
    "        None: The test does not return a value but uses assertions to ensure the correctness of the function being tested.\n",
    "\n",
    "        The function first creates a pandas DataFrame 'entrada' with sample text, some containing hyperlinks.\n",
    "        It then defines what the expected result should look like in the DataFrame 'esperado', where the hyperlink is removed.\n",
    "        'resultado' stores the output from the function 'extract_links_df', which is expected to remove links from the 'entrada' DataFrame.\n",
    "        Finally, the function asserts that 'resultado' matches 'esperado'. If there is a mismatch, it raises an assertion error with the message \"Links were not removed correctly\".\n",
    "    \"\"\"\n",
    "    entrada = pd.DataFrame({\"comment\": [\"Check this out: http://example.com\", \"No link here\"]})\n",
    "    esperado = pd.DataFrame({\"comment\": [\"Check this out: \", \"No link here\"]})\n",
    "    resultado = extract_links_df(entrada)\n",
    "    assert resultado.equals(esperado), \"Links were not removed correctly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_to_lower():\n",
    "    \"\"\"\n",
    "        Tests the to_lower_df function to verify if it successfully converts all letters in strings within a DataFrame column to lowercase.\n",
    "        Args:\n",
    "        None: The test does not require external arguments as the input data is defined within the function.\n",
    "\n",
    "        Returns:\n",
    "        None: The test does not return a value but uses assertions to ensure the functionality of the function being tested.\n",
    "\n",
    "        The function begins by creating a pandas DataFrame 'entrada' with some sample text where some are in uppercase.\n",
    "        It then defines what the expected result should look like in the DataFrame 'esperado', which contains the same text but entirely in lowercase.\n",
    "        'resultado' stores the output from the function 'to_lower_df', which is expected to convert all uppercase letters in the 'entrada' DataFrame to lowercase.\n",
    "        Finally, the function asserts that 'resultado' matches 'esperado'. If there is a mismatch, it raises an assertion error with the message \"Conversion to lowercase failed\".\n",
    "    \"\"\"\n",
    "    entrada = pd.DataFrame({\"comment\": [\"HELLO WORLD\", \"Already lowercase\"]})\n",
    "    esperado = pd.DataFrame({\"comment\": [\"hello world\", \"already lowercase\"]})\n",
    "    resultado = to_lower_df(entrada)\n",
    "    assert resultado.equals(esperado), \"Conversion to lowercase failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenize_and_pre_processing():\n",
    "    \"\"\"\n",
    "        Tests the tokenize_and_pre_processing_df function to verify if it correctly tokenizes the comments\n",
    "        in a DataFrame and applies the expected pre-processing steps.\n",
    "        Args:\n",
    "        None: The test does not require external arguments as the input data is defined within the function.\n",
    "\n",
    "        Returns:\n",
    "        None: The test does not return a value but uses assertions to ensure the functionality of the function being tested.\n",
    "\n",
    "        This function initiates by creating a pandas DataFrame 'entrada' with sample text comments.\n",
    "        It then defines 'esperado', a DataFrame that outlines the expected result of tokenized lists of words.\n",
    "        'resultado' captures the output from 'tokenize_and_pre_processing_df', which is expected to tokenize and preprocess the text in 'entrada'.\n",
    "        Finally, the function asserts that 'resultado' matches 'esperado', raising an assertion error with the message \"Tokenization or pre-processing did not function as expected\" if the output is incorrect.\n",
    "    \"\"\"\n",
    "    entrada = pd.DataFrame({\"comment\": [\"hello world\", \"testing pytest\"]})\n",
    "    esperado = pd.DataFrame({\"tokens\": [[\"hello\", \"world\"], [\"testing\", \"pytest\"]]})\n",
    "    resultado = tokenize_and_pre_processing_df(entrada)\n",
    "    assert resultado.equals(esperado), \"Tokenization or pre-processing did not function as expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `preprocess_pipeline` centraliza todas as operações de pré-processamento em um único local, facilitando assim a sua aplicação em diferentes DataFrames. Este método permite uma integração eficiente e organizada dos passos necessários para preparar os dados, tornando o processo de aplicação das funções de pré-processamento mais simples e direto em novos conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(df):\n",
    "    \"\"\"\n",
    "        The pipeline of the pre_processing\n",
    "        Args:\n",
    "        Dataframe with a column called 'comment'\n",
    "\n",
    "        Returns:\n",
    "        The modified dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spacy.cli.download('en_core_web_lg')\n",
    "    except:\n",
    "        pass\n",
    "    remove_links = extract_links_df(df)\n",
    "\n",
    "    text_lower = to_lower_df(remove_links)\n",
    "\n",
    "    pos_tokens = tokenize_and_pre_processing_df(text_lower)\n",
    "\n",
    "    return pos_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagem do Pipeline\n",
    "\n",
    "A imagem do pipeline se encontra na documentação do projeto, dentro da seção '5' de pré-processamento e dentro dela, no tópico '5.5'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código tem como funcionalidade exportar o novo arquivo csv gerado pelo pipeline para a pasta estabelecida no `export_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isAtGoogleColab:\n",
    "    df = pd.read_csv('/content/classification-labeled.csv', encoding='latin1', delimiter=';')\n",
    "    export_path = '/content/processed_classification-labeled.csv'\n",
    "else:\n",
    "    df = pd.read_csv('./data/classification-labeled.csv', encoding='latin1', delimiter=';')\n",
    "    export_path = './data/processed_classification-labeled.csv'\n",
    "\n",
    "processed_text = preprocess_pipeline(df)\n",
    "\n",
    "columns_to_keep = ['id', 'comment', 'links', 'sentiment', 'pos_tokens']  \n",
    "\n",
    "#Create a new dataframe\n",
    "df_final = processed_text[columns_to_keep]\n",
    "df_final.to_csv(export_path, index=False)\n",
    "\n",
    "print(f\"DataFrame processado e ajustado salvo em: {export_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
